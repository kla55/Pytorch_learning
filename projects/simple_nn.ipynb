{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPcoxe1MQxhoSVXxxDRZM7K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kla55/Pytorch_learning/blob/main/projects/simple_nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "import torch\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import optim"
      ],
      "metadata": {
        "id": "QlnB7LRTPNeq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "lHtHL7C7c4ZQ",
        "outputId": "77e4b867-4c44-4b0b-a312-7cd644c78385"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torchvision.transforms import ToTensor, Resize, Compose\n",
        "\n",
        "\n",
        "transformations = Compose([\n",
        "    Resize((28, 28)),  # Resize each image to 256x256\n",
        "    ToTensor()           # Then convert them to Tensor\n",
        "])"
      ],
      "metadata": {
        "id": "o3a2hpGJrWOS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = datasets.Food101(\n",
        "    root=\"data\",\n",
        "    split='train',\n",
        "    download=True,\n",
        "    transform=transformations\n",
        ")\n",
        "\n",
        "test_data = datasets.Food101(\n",
        "    root=\"data\",\n",
        "    split='test',\n",
        "    download=True,\n",
        "    transform=transformations\n",
        ")"
      ],
      "metadata": {
        "id": "ZtSUArA9PNhf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "590b355d-63d7-4566-8009-76d286df2a24"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://data.vision.ee.ethz.ch/cvl/food-101.tar.gz to data/food-101.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|â–Ž         | 177635328/4996278331 [00:11<05:24, 14846910.71it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-9c7a5a3b2467>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_data = datasets.Food101(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransformations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/food101.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, split, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/food101.py\u001b[0m in \u001b[0;36m_download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mdownload_and_extract_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_URL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_root\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_MD5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_and_extract_archive\u001b[0;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[1;32m    432\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m     \u001b[0mdownload_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[0marchive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_url\u001b[0;34m(url, root, filename, md5, max_redirect_hops)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Downloading \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" to \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0m_urlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mURLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"https\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36m_urlretrieve\u001b[0;34m(url, filename, chunk_size)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_urlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1024\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"User-Agent\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUSER_AGENT\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0m_save_response_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36m_save_response_content\u001b[0;34m(content, destination, length)\u001b[0m\n\u001b[1;32m     35\u001b[0m ) -> None:\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestination\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0;31m# filter out keep-alive new chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_urlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1024\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"User-Agent\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUSER_AGENT\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0m_save_response_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    464\u001b[0m                 \u001b[0;31m# clip the read to the \"end of response\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m                 \u001b[0mamt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m                 \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1303\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1305\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1157\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1159\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = len(train_data.classes)\n",
        "num_classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frnP0Ut-1sxH",
        "outputId": "7e9969ea-8430-4e1c-cc97-579a9190bc54"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "101"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "fig = plt.figure(figsize=(10,15))\n",
        "rows, cols = 1, 5\n",
        "for i in range(1, 6):\n",
        "  random_inx = torch.randint(0,len(train_data), size =[1]).item()\n",
        "  print(random_inx)\n",
        "  image, label = train_data[random_inx]\n",
        "  image = np.transpose(image, (1, 2, 0))\n",
        "  fig.add_subplot(rows, cols, i)\n",
        "  plt.imshow(image.squeeze(), cmap = 'gray')\n",
        "  plt.title(train_data.classes[label])\n",
        "  plt.axis(False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "bY1dMG680Rp1",
        "outputId": "4251e007-35ce-4a9d-e3c3-0bd2476105f5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "27889\n",
            "60213\n",
            "26420\n",
            "59523\n",
            "69208\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1500 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_dataloader = DataLoader(train_data,\n",
        "                              batch_size = BATCH_SIZE,\n",
        "                              shuffle = True)\n",
        "\n",
        "test_dataloader = DataLoader(test_data,\n",
        "                             batch_size = BATCH_SIZE,\n",
        "                             shuffle = True)\n",
        "\n",
        "print(f\"Dataloaders: {train_dataloader, test_dataloader}\")\n",
        "print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\n",
        "print(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibngZ30_E7Af",
        "outputId": "ea074d57-396b-4599-f50f-54a6b7ff6602"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataloaders: (<torch.utils.data.dataloader.DataLoader object at 0x78c40fd3b160>, <torch.utils.data.dataloader.DataLoader object at 0x78c40fd3b1c0>)\n",
            "Length of train dataloader: 2368 batches of 32\n",
            "Length of test dataloader: 790 batches of 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_features, train_labels = next(iter(train_data))"
      ],
      "metadata": {
        "id": "T6LJ0Y_abIjO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_features.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bseOvHOca4gl",
        "outputId": "f4c97c8d-173f-4dd5-b4ef-687466934892"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "class model_v0(torch.nn.Module):\n",
        "  def __init__(self, input_shape: int,\n",
        "               hidden_units: int,\n",
        "               output_shape: int):\n",
        "    super().__init__()\n",
        "    self.conv_block_1 = nn.Sequential(\n",
        "                    nn.Conv2d(in_channels=input_shape,\n",
        "                              out_channels=hidden_units,\n",
        "                              kernel_size=3,\n",
        "                              stride=1,\n",
        "                              padding=1),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Conv2d(in_channels=hidden_units,\n",
        "                        out_channels=hidden_units,\n",
        "                        kernel_size=3,\n",
        "                        stride=1,\n",
        "                        padding=1),\n",
        "                    nn.ReLU(),\n",
        "                    nn.MaxPool2d(kernel_size=2))\n",
        "\n",
        "    self.conv_block_2 = nn.Sequential(\n",
        "                      nn.Conv2d(in_channels=hidden_units,\n",
        "                                out_channels=hidden_units,\n",
        "                                kernel_size=3,\n",
        "                                stride=1,\n",
        "                                padding=1),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Conv2d(in_channels=hidden_units,\n",
        "                                out_channels=hidden_units,\n",
        "                                kernel_size=3,\n",
        "                                stride=1,\n",
        "                                padding=1),\n",
        "                      nn.ReLU(),\n",
        "                      nn.MaxPool2d(kernel_size=2))\n",
        "\n",
        "    self.classifier = nn.Sequential(\n",
        "                    nn.Flatten(),\n",
        "                    nn.Linear(in_features=hidden_units*7*7,\n",
        "                              out_features=output_shape))\n",
        "\n",
        "  def forward(self, x):\n",
        "                    x = self.conv_block_1(x)\n",
        "                    # print(f\"Output shape of conv block 1: {x.shape}\")\n",
        "                    x = self.conv_block_2(x)\n",
        "                    # print(f\"Output shape of conv block 2: {x.shape}\")\n",
        "                    x = self.classifier(x)\n",
        "                    # print(f\"Output shape of classifier: {x.shape}\")\n",
        "                    return x"
      ],
      "metadata": {
        "id": "svI8yWlAa4jJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_V0 = model_v0(input_shape=3,\n",
        "                  hidden_units=10,\n",
        "                  output_shape=101).to(device)\n",
        "model_V0\n",
        "\n"
      ],
      "metadata": {
        "id": "aBC0Oyc-a4lv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_x = torch.rand(size = (3, 28, 28)).unsqueeze( dim = 0).to(device)\n",
        "model_V0(dummy_x)\n"
      ],
      "metadata": {
        "id": "r7J7hrPqjfhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "model_cpu = model_v0(input_shape=3,\n",
        "                    hidden_units=10,\n",
        "                    output_shape=101)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model_cpu.parameters(), lr=0.1)"
      ],
      "metadata": {
        "id": "dhRMXXV6EgJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 5\n",
        "\n",
        "for epoch in tqdm(range(EPOCHS)):\n",
        "  train_loss = 0\n",
        "  model_cpu.train()\n",
        "  for batch, (X, y) in enumerate(train_dataloader):\n",
        "    X, y = X.to('cpu'), y.to('cpu')\n",
        "    y_pred = model_cpu(X)\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    train_loss += loss\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  train_loss /= len(train_dataloader)\n",
        "\n",
        "  test_loss_total = 0\n",
        "  model_cpu.eval()\n",
        "  with torch.inference_mode():\n",
        "    for batch, (X_test, y_test) in enumerate(test_dataloader):\n",
        "      X_test, y_test = X_test.to(\"cpu\"), y_test.to(\"cpu\")\n",
        "      test_pred = model_cpu(X_test)\n",
        "      test_loss = loss_fn(test_pred, y_test)\n",
        "      test_loss_total += test_loss\n",
        "    test_loss_total /= len(test_dataloader)\n",
        "\n",
        "  print(f\"Epoch: {epoch} | Loss: {train_loss:.3f} | Test loss: {test_loss_total:.3f}\")"
      ],
      "metadata": {
        "id": "Ai95bZf7-_zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "import torch"
      ],
      "metadata": {
        "id": "ZMkyZsivx2TU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with resnet\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "\n",
        "model = models.resnet101(pretrained=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGdq_6P65lQU",
        "outputId": "fd2a83cd-e778-4060-f476-9f913da7e07b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 171M/171M [00:01<00:00, 102MB/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "in_features = int(model.fc.in_features)\n",
        "model.fc = nn.Linear(in_features, 101, device)"
      ],
      "metadata": {
        "id": "R074JxBy0Rsn"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "bdNAemeH0RvL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "lr_scheduler = optim.lr_scheduler.StepLR(optimizer,step_size=5, gamma=0.1)"
      ],
      "metadata": {
        "id": "2VLJBmDf0R0j"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "nkhxOwazwwre",
        "outputId": "0da171ff-6897-43b5-b4ca-0e7bf9045415"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.Food101(\n",
        "    root=\"data\",\n",
        "    split='train',\n",
        "    download=True,\n",
        "    transform=transformations\n",
        ")\n",
        "\n",
        "evaluation_dataset = datasets.Food101(\n",
        "    root=\"data\",\n",
        "    split='test',\n",
        "    download=True,\n",
        "    transform=transformations\n",
        ")"
      ],
      "metadata": {
        "id": "gZ53H5dcw9bz"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "image_size = 224\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size = 32, shuffle=True, num_workers=2)\n",
        "\n",
        "evaluation_dataloader = DataLoader(evaluation_dataset, batch_size = 32, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "MfKA1Xrsw11T"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "num_epochs = 15\n",
        "losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i , (inputs, targets) in enumerate(train_dataloader):\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        #train model\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        #criterion\n",
        "        loss = criterion(outputs, targets)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        #backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        #update parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # report\n",
        "        if (i + 1) % 50 == 0:\n",
        "            print('Epoch [%2d/%2d], Step [%3d/%3d], Loss: %.4f'\n",
        "                  % (epoch + 1, num_epochs, i + 1, len(train_dataset) // batch_size, loss.item()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmHCmAP_w5N6",
        "outputId": "00c52c32-a746-494d-b4c8-521864002d6f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [ 1/15], Step [ 50/2367], Loss: 4.5520\n",
            "Epoch [ 1/15], Step [100/2367], Loss: 4.6579\n",
            "Epoch [ 1/15], Step [150/2367], Loss: 4.4761\n",
            "Epoch [ 1/15], Step [200/2367], Loss: 4.4928\n",
            "Epoch [ 1/15], Step [250/2367], Loss: 4.3435\n",
            "Epoch [ 1/15], Step [300/2367], Loss: 4.5102\n",
            "Epoch [ 1/15], Step [350/2367], Loss: 4.1765\n",
            "Epoch [ 1/15], Step [400/2367], Loss: 4.1593\n",
            "Epoch [ 1/15], Step [450/2367], Loss: 4.0115\n",
            "Epoch [ 1/15], Step [500/2367], Loss: 3.8930\n",
            "Epoch [ 1/15], Step [550/2367], Loss: 3.7929\n",
            "Epoch [ 1/15], Step [600/2367], Loss: 3.8983\n",
            "Epoch [ 1/15], Step [650/2367], Loss: 3.8615\n",
            "Epoch [ 1/15], Step [700/2367], Loss: 4.1762\n",
            "Epoch [ 1/15], Step [750/2367], Loss: 3.8436\n",
            "Epoch [ 1/15], Step [800/2367], Loss: 3.6285\n",
            "Epoch [ 1/15], Step [850/2367], Loss: 3.9637\n",
            "Epoch [ 1/15], Step [900/2367], Loss: 3.5182\n",
            "Epoch [ 1/15], Step [950/2367], Loss: 3.6933\n",
            "Epoch [ 1/15], Step [1000/2367], Loss: 3.9629\n",
            "Epoch [ 1/15], Step [1050/2367], Loss: 4.1833\n",
            "Epoch [ 1/15], Step [1100/2367], Loss: 3.9124\n",
            "Epoch [ 1/15], Step [1150/2367], Loss: 3.7508\n",
            "Epoch [ 1/15], Step [1200/2367], Loss: 3.9942\n",
            "Epoch [ 1/15], Step [1250/2367], Loss: 3.5208\n",
            "Epoch [ 1/15], Step [1300/2367], Loss: 3.7807\n",
            "Epoch [ 1/15], Step [1350/2367], Loss: 3.9479\n",
            "Epoch [ 1/15], Step [1400/2367], Loss: 3.8115\n",
            "Epoch [ 1/15], Step [1450/2367], Loss: 3.4640\n",
            "Epoch [ 1/15], Step [1500/2367], Loss: 3.7016\n",
            "Epoch [ 1/15], Step [1550/2367], Loss: 3.9255\n",
            "Epoch [ 1/15], Step [1600/2367], Loss: 3.6204\n",
            "Epoch [ 1/15], Step [1650/2367], Loss: 3.7037\n",
            "Epoch [ 1/15], Step [1700/2367], Loss: 3.3229\n",
            "Epoch [ 1/15], Step [1750/2367], Loss: 3.2753\n",
            "Epoch [ 1/15], Step [1800/2367], Loss: 3.3237\n",
            "Epoch [ 1/15], Step [1850/2367], Loss: 3.4857\n",
            "Epoch [ 1/15], Step [1900/2367], Loss: 3.5013\n",
            "Epoch [ 1/15], Step [1950/2367], Loss: 3.8160\n",
            "Epoch [ 1/15], Step [2000/2367], Loss: 3.5656\n",
            "Epoch [ 1/15], Step [2050/2367], Loss: 3.5834\n",
            "Epoch [ 1/15], Step [2100/2367], Loss: 3.5470\n",
            "Epoch [ 1/15], Step [2150/2367], Loss: 3.5068\n",
            "Epoch [ 1/15], Step [2200/2367], Loss: 3.4289\n",
            "Epoch [ 1/15], Step [2250/2367], Loss: 3.6104\n",
            "Epoch [ 1/15], Step [2300/2367], Loss: 3.5436\n",
            "Epoch [ 1/15], Step [2350/2367], Loss: 3.8312\n",
            "Epoch [ 2/15], Step [ 50/2367], Loss: 3.5090\n",
            "Epoch [ 2/15], Step [100/2367], Loss: 3.3310\n",
            "Epoch [ 2/15], Step [150/2367], Loss: 2.7673\n",
            "Epoch [ 2/15], Step [200/2367], Loss: 3.3258\n",
            "Epoch [ 2/15], Step [250/2367], Loss: 3.5035\n",
            "Epoch [ 2/15], Step [300/2367], Loss: 3.1275\n",
            "Epoch [ 2/15], Step [350/2367], Loss: 3.4634\n",
            "Epoch [ 2/15], Step [400/2367], Loss: 3.0008\n",
            "Epoch [ 2/15], Step [450/2367], Loss: 3.3715\n",
            "Epoch [ 2/15], Step [500/2367], Loss: 3.3498\n",
            "Epoch [ 2/15], Step [550/2367], Loss: 3.3972\n",
            "Epoch [ 2/15], Step [600/2367], Loss: 3.6271\n",
            "Epoch [ 2/15], Step [650/2367], Loss: 3.0876\n",
            "Epoch [ 2/15], Step [700/2367], Loss: 3.3096\n",
            "Epoch [ 2/15], Step [750/2367], Loss: 3.0406\n",
            "Epoch [ 2/15], Step [800/2367], Loss: 3.7547\n",
            "Epoch [ 2/15], Step [850/2367], Loss: 3.1873\n",
            "Epoch [ 2/15], Step [900/2367], Loss: 3.1114\n",
            "Epoch [ 2/15], Step [950/2367], Loss: 3.0890\n",
            "Epoch [ 2/15], Step [1000/2367], Loss: 3.8948\n",
            "Epoch [ 2/15], Step [1050/2367], Loss: 3.2834\n",
            "Epoch [ 2/15], Step [1100/2367], Loss: 2.8289\n",
            "Epoch [ 2/15], Step [1150/2367], Loss: 3.2426\n",
            "Epoch [ 2/15], Step [1200/2367], Loss: 3.2294\n",
            "Epoch [ 2/15], Step [1250/2367], Loss: 3.3176\n",
            "Epoch [ 2/15], Step [1300/2367], Loss: 3.1067\n",
            "Epoch [ 2/15], Step [1350/2367], Loss: 3.2892\n",
            "Epoch [ 2/15], Step [1400/2367], Loss: 3.1948\n",
            "Epoch [ 2/15], Step [1450/2367], Loss: 4.0253\n",
            "Epoch [ 2/15], Step [1500/2367], Loss: 3.2459\n",
            "Epoch [ 2/15], Step [1550/2367], Loss: 3.1198\n",
            "Epoch [ 2/15], Step [1600/2367], Loss: 3.7739\n",
            "Epoch [ 2/15], Step [1650/2367], Loss: 2.9797\n",
            "Epoch [ 2/15], Step [1700/2367], Loss: 2.6471\n",
            "Epoch [ 2/15], Step [1750/2367], Loss: 3.3966\n",
            "Epoch [ 2/15], Step [1800/2367], Loss: 3.3220\n",
            "Epoch [ 2/15], Step [1850/2367], Loss: 2.9748\n",
            "Epoch [ 2/15], Step [1900/2367], Loss: 3.3019\n",
            "Epoch [ 2/15], Step [1950/2367], Loss: 2.8330\n",
            "Epoch [ 2/15], Step [2000/2367], Loss: 3.1727\n",
            "Epoch [ 2/15], Step [2050/2367], Loss: 2.8330\n",
            "Epoch [ 2/15], Step [2100/2367], Loss: 3.4478\n",
            "Epoch [ 2/15], Step [2150/2367], Loss: 3.7427\n",
            "Epoch [ 2/15], Step [2200/2367], Loss: 3.0643\n",
            "Epoch [ 2/15], Step [2250/2367], Loss: 3.0883\n",
            "Epoch [ 2/15], Step [2300/2367], Loss: 3.1302\n",
            "Epoch [ 2/15], Step [2350/2367], Loss: 3.4094\n",
            "Epoch [ 3/15], Step [ 50/2367], Loss: 2.9114\n",
            "Epoch [ 3/15], Step [100/2367], Loss: 3.0019\n",
            "Epoch [ 3/15], Step [150/2367], Loss: 3.1745\n",
            "Epoch [ 3/15], Step [200/2367], Loss: 3.1543\n",
            "Epoch [ 3/15], Step [250/2367], Loss: 3.7809\n",
            "Epoch [ 3/15], Step [300/2367], Loss: 3.5457\n",
            "Epoch [ 3/15], Step [350/2367], Loss: 2.6306\n",
            "Epoch [ 3/15], Step [400/2367], Loss: 3.3841\n",
            "Epoch [ 3/15], Step [450/2367], Loss: 3.3326\n",
            "Epoch [ 3/15], Step [500/2367], Loss: 2.8120\n",
            "Epoch [ 3/15], Step [550/2367], Loss: 3.5848\n",
            "Epoch [ 3/15], Step [600/2367], Loss: 3.0701\n",
            "Epoch [ 3/15], Step [650/2367], Loss: 3.0669\n",
            "Epoch [ 3/15], Step [700/2367], Loss: 3.4273\n",
            "Epoch [ 3/15], Step [750/2367], Loss: 3.0399\n",
            "Epoch [ 3/15], Step [800/2367], Loss: 3.5920\n",
            "Epoch [ 3/15], Step [850/2367], Loss: 3.4455\n",
            "Epoch [ 3/15], Step [900/2367], Loss: 3.2474\n",
            "Epoch [ 3/15], Step [950/2367], Loss: 2.9850\n",
            "Epoch [ 3/15], Step [1000/2367], Loss: 3.2801\n",
            "Epoch [ 3/15], Step [1050/2367], Loss: 3.4396\n",
            "Epoch [ 3/15], Step [1100/2367], Loss: 3.7065\n",
            "Epoch [ 3/15], Step [1150/2367], Loss: 2.9951\n",
            "Epoch [ 3/15], Step [1200/2367], Loss: 3.0835\n",
            "Epoch [ 3/15], Step [1250/2367], Loss: 3.9417\n",
            "Epoch [ 3/15], Step [1300/2367], Loss: 2.6445\n",
            "Epoch [ 3/15], Step [1350/2367], Loss: 3.0735\n",
            "Epoch [ 3/15], Step [1400/2367], Loss: 3.1026\n",
            "Epoch [ 3/15], Step [1450/2367], Loss: 3.1603\n",
            "Epoch [ 3/15], Step [1500/2367], Loss: 2.7951\n",
            "Epoch [ 3/15], Step [1550/2367], Loss: 3.6826\n",
            "Epoch [ 3/15], Step [1600/2367], Loss: 3.1475\n",
            "Epoch [ 3/15], Step [1650/2367], Loss: 3.0248\n",
            "Epoch [ 3/15], Step [1700/2367], Loss: 3.2282\n",
            "Epoch [ 3/15], Step [1750/2367], Loss: 3.7817\n",
            "Epoch [ 3/15], Step [1800/2367], Loss: 3.5458\n",
            "Epoch [ 3/15], Step [1850/2367], Loss: 3.9290\n",
            "Epoch [ 3/15], Step [1900/2367], Loss: 3.5160\n",
            "Epoch [ 3/15], Step [1950/2367], Loss: 3.2982\n",
            "Epoch [ 3/15], Step [2000/2367], Loss: 3.7747\n",
            "Epoch [ 3/15], Step [2050/2367], Loss: 2.9842\n",
            "Epoch [ 3/15], Step [2100/2367], Loss: 3.0058\n",
            "Epoch [ 3/15], Step [2150/2367], Loss: 3.3820\n",
            "Epoch [ 3/15], Step [2200/2367], Loss: 3.4169\n",
            "Epoch [ 3/15], Step [2250/2367], Loss: 2.9137\n",
            "Epoch [ 3/15], Step [2300/2367], Loss: 3.3686\n",
            "Epoch [ 3/15], Step [2350/2367], Loss: 3.7371\n",
            "Epoch [ 4/15], Step [ 50/2367], Loss: 3.1315\n",
            "Epoch [ 4/15], Step [100/2367], Loss: 3.4614\n",
            "Epoch [ 4/15], Step [150/2367], Loss: 3.0415\n",
            "Epoch [ 4/15], Step [200/2367], Loss: 3.1176\n",
            "Epoch [ 4/15], Step [250/2367], Loss: 3.2919\n",
            "Epoch [ 4/15], Step [300/2367], Loss: 2.6034\n",
            "Epoch [ 4/15], Step [350/2367], Loss: 2.7015\n",
            "Epoch [ 4/15], Step [400/2367], Loss: 2.5200\n",
            "Epoch [ 4/15], Step [450/2367], Loss: 2.6700\n",
            "Epoch [ 4/15], Step [500/2367], Loss: 3.0359\n",
            "Epoch [ 4/15], Step [550/2367], Loss: 3.1942\n",
            "Epoch [ 4/15], Step [600/2367], Loss: 2.6812\n",
            "Epoch [ 4/15], Step [650/2367], Loss: 3.1537\n",
            "Epoch [ 4/15], Step [700/2367], Loss: 2.6558\n",
            "Epoch [ 4/15], Step [750/2367], Loss: 2.9735\n",
            "Epoch [ 4/15], Step [800/2367], Loss: 2.9746\n",
            "Epoch [ 4/15], Step [850/2367], Loss: 2.6817\n",
            "Epoch [ 4/15], Step [900/2367], Loss: 3.3764\n",
            "Epoch [ 4/15], Step [950/2367], Loss: 2.4446\n",
            "Epoch [ 4/15], Step [1000/2367], Loss: 3.8492\n",
            "Epoch [ 4/15], Step [1050/2367], Loss: 3.2870\n",
            "Epoch [ 4/15], Step [1100/2367], Loss: 3.0420\n",
            "Epoch [ 4/15], Step [1150/2367], Loss: 3.2401\n",
            "Epoch [ 4/15], Step [1200/2367], Loss: 3.8858\n",
            "Epoch [ 4/15], Step [1250/2367], Loss: 3.7626\n",
            "Epoch [ 4/15], Step [1300/2367], Loss: 3.5792\n",
            "Epoch [ 4/15], Step [1350/2367], Loss: 3.5010\n",
            "Epoch [ 4/15], Step [1400/2367], Loss: 3.2741\n",
            "Epoch [ 4/15], Step [1450/2367], Loss: 3.6284\n",
            "Epoch [ 4/15], Step [1500/2367], Loss: 3.3940\n",
            "Epoch [ 4/15], Step [1550/2367], Loss: 3.4187\n",
            "Epoch [ 4/15], Step [1600/2367], Loss: 3.0637\n",
            "Epoch [ 4/15], Step [1650/2367], Loss: 4.0626\n",
            "Epoch [ 4/15], Step [1700/2367], Loss: 4.0812\n",
            "Epoch [ 4/15], Step [1750/2367], Loss: 3.8939\n",
            "Epoch [ 4/15], Step [1800/2367], Loss: 3.3673\n",
            "Epoch [ 4/15], Step [1850/2367], Loss: 3.9339\n",
            "Epoch [ 4/15], Step [1900/2367], Loss: 3.2642\n",
            "Epoch [ 4/15], Step [1950/2367], Loss: 3.5370\n",
            "Epoch [ 4/15], Step [2000/2367], Loss: 3.3663\n",
            "Epoch [ 4/15], Step [2050/2367], Loss: 3.1893\n",
            "Epoch [ 4/15], Step [2100/2367], Loss: 3.1508\n",
            "Epoch [ 4/15], Step [2150/2367], Loss: 3.0797\n",
            "Epoch [ 4/15], Step [2200/2367], Loss: 3.0916\n",
            "Epoch [ 4/15], Step [2250/2367], Loss: 3.7450\n",
            "Epoch [ 4/15], Step [2300/2367], Loss: 3.3236\n",
            "Epoch [ 4/15], Step [2350/2367], Loss: 3.0797\n",
            "Epoch [ 5/15], Step [ 50/2367], Loss: 3.2562\n",
            "Epoch [ 5/15], Step [100/2367], Loss: 3.2005\n",
            "Epoch [ 5/15], Step [150/2367], Loss: 2.8656\n",
            "Epoch [ 5/15], Step [200/2367], Loss: 2.3374\n",
            "Epoch [ 5/15], Step [250/2367], Loss: 2.5018\n",
            "Epoch [ 5/15], Step [300/2367], Loss: 2.3910\n",
            "Epoch [ 5/15], Step [350/2367], Loss: 2.8190\n",
            "Epoch [ 5/15], Step [400/2367], Loss: 3.4198\n",
            "Epoch [ 5/15], Step [450/2367], Loss: 3.3791\n",
            "Epoch [ 5/15], Step [500/2367], Loss: 3.8231\n",
            "Epoch [ 5/15], Step [550/2367], Loss: 2.5888\n",
            "Epoch [ 5/15], Step [600/2367], Loss: 3.4417\n",
            "Epoch [ 5/15], Step [650/2367], Loss: 3.8806\n",
            "Epoch [ 5/15], Step [700/2367], Loss: 3.4181\n",
            "Epoch [ 5/15], Step [750/2367], Loss: 3.3725\n",
            "Epoch [ 5/15], Step [800/2367], Loss: 2.7022\n",
            "Epoch [ 5/15], Step [850/2367], Loss: 2.7611\n",
            "Epoch [ 5/15], Step [900/2367], Loss: 3.5707\n",
            "Epoch [ 5/15], Step [950/2367], Loss: 2.9947\n",
            "Epoch [ 5/15], Step [1000/2367], Loss: 3.2495\n",
            "Epoch [ 5/15], Step [1050/2367], Loss: 2.8501\n",
            "Epoch [ 5/15], Step [1100/2367], Loss: 3.5672\n",
            "Epoch [ 5/15], Step [1150/2367], Loss: 2.8305\n",
            "Epoch [ 5/15], Step [1200/2367], Loss: 3.2164\n",
            "Epoch [ 5/15], Step [1250/2367], Loss: 3.0668\n",
            "Epoch [ 5/15], Step [1300/2367], Loss: 2.6889\n",
            "Epoch [ 5/15], Step [1350/2367], Loss: 3.0887\n",
            "Epoch [ 5/15], Step [1400/2367], Loss: 2.6927\n",
            "Epoch [ 5/15], Step [1450/2367], Loss: 2.9240\n",
            "Epoch [ 5/15], Step [1500/2367], Loss: 3.3497\n",
            "Epoch [ 5/15], Step [1550/2367], Loss: 2.4672\n",
            "Epoch [ 5/15], Step [1600/2367], Loss: 2.9385\n",
            "Epoch [ 5/15], Step [1650/2367], Loss: 2.8443\n",
            "Epoch [ 5/15], Step [1700/2367], Loss: 2.8416\n",
            "Epoch [ 5/15], Step [1750/2367], Loss: 2.5902\n",
            "Epoch [ 5/15], Step [1800/2367], Loss: 3.1109\n",
            "Epoch [ 5/15], Step [1850/2367], Loss: 3.3331\n",
            "Epoch [ 5/15], Step [1900/2367], Loss: 2.8940\n",
            "Epoch [ 5/15], Step [1950/2367], Loss: 2.8446\n",
            "Epoch [ 5/15], Step [2000/2367], Loss: 3.1715\n",
            "Epoch [ 5/15], Step [2050/2367], Loss: 3.2944\n",
            "Epoch [ 5/15], Step [2100/2367], Loss: 3.0297\n",
            "Epoch [ 5/15], Step [2150/2367], Loss: 2.9219\n",
            "Epoch [ 5/15], Step [2200/2367], Loss: 2.9634\n",
            "Epoch [ 5/15], Step [2250/2367], Loss: 3.1686\n",
            "Epoch [ 5/15], Step [2300/2367], Loss: 2.4873\n",
            "Epoch [ 5/15], Step [2350/2367], Loss: 2.9475\n",
            "Epoch [ 6/15], Step [ 50/2367], Loss: 2.0749\n",
            "Epoch [ 6/15], Step [100/2367], Loss: 2.4690\n",
            "Epoch [ 6/15], Step [150/2367], Loss: 2.9293\n",
            "Epoch [ 6/15], Step [200/2367], Loss: 2.3538\n",
            "Epoch [ 6/15], Step [250/2367], Loss: 2.8076\n",
            "Epoch [ 6/15], Step [300/2367], Loss: 2.9246\n",
            "Epoch [ 6/15], Step [350/2367], Loss: 2.4163\n",
            "Epoch [ 6/15], Step [400/2367], Loss: 3.1892\n",
            "Epoch [ 6/15], Step [450/2367], Loss: 3.1320\n",
            "Epoch [ 6/15], Step [500/2367], Loss: 3.0543\n",
            "Epoch [ 6/15], Step [550/2367], Loss: 2.5553\n",
            "Epoch [ 6/15], Step [600/2367], Loss: 2.8888\n",
            "Epoch [ 6/15], Step [650/2367], Loss: 3.2961\n",
            "Epoch [ 6/15], Step [700/2367], Loss: 2.9301\n",
            "Epoch [ 6/15], Step [750/2367], Loss: 2.7256\n",
            "Epoch [ 6/15], Step [800/2367], Loss: 3.0029\n",
            "Epoch [ 6/15], Step [850/2367], Loss: 2.5985\n",
            "Epoch [ 6/15], Step [900/2367], Loss: 2.8118\n",
            "Epoch [ 6/15], Step [950/2367], Loss: 2.7928\n",
            "Epoch [ 6/15], Step [1000/2367], Loss: 2.6504\n",
            "Epoch [ 6/15], Step [1050/2367], Loss: 2.6519\n",
            "Epoch [ 6/15], Step [1100/2367], Loss: 2.9693\n",
            "Epoch [ 6/15], Step [1150/2367], Loss: 2.9416\n",
            "Epoch [ 6/15], Step [1200/2367], Loss: 3.0190\n",
            "Epoch [ 6/15], Step [1250/2367], Loss: 2.5674\n",
            "Epoch [ 6/15], Step [1300/2367], Loss: 2.6349\n",
            "Epoch [ 6/15], Step [1350/2367], Loss: 2.9485\n",
            "Epoch [ 6/15], Step [1400/2367], Loss: 2.7115\n",
            "Epoch [ 6/15], Step [1450/2367], Loss: 2.9788\n",
            "Epoch [ 6/15], Step [1500/2367], Loss: 2.8928\n",
            "Epoch [ 6/15], Step [1550/2367], Loss: 2.4911\n",
            "Epoch [ 6/15], Step [1600/2367], Loss: 2.8241\n",
            "Epoch [ 6/15], Step [1650/2367], Loss: 2.4056\n",
            "Epoch [ 6/15], Step [1700/2367], Loss: 2.9988\n",
            "Epoch [ 6/15], Step [1750/2367], Loss: 3.3119\n",
            "Epoch [ 6/15], Step [1800/2367], Loss: 2.6208\n",
            "Epoch [ 6/15], Step [1850/2367], Loss: 3.3297\n",
            "Epoch [ 6/15], Step [1900/2367], Loss: 2.5743\n",
            "Epoch [ 6/15], Step [1950/2367], Loss: 2.8134\n",
            "Epoch [ 6/15], Step [2000/2367], Loss: 3.0182\n",
            "Epoch [ 6/15], Step [2050/2367], Loss: 3.2108\n",
            "Epoch [ 6/15], Step [2100/2367], Loss: 3.3040\n",
            "Epoch [ 6/15], Step [2150/2367], Loss: 2.9777\n",
            "Epoch [ 6/15], Step [2200/2367], Loss: 2.5978\n",
            "Epoch [ 6/15], Step [2250/2367], Loss: 3.1533\n",
            "Epoch [ 6/15], Step [2300/2367], Loss: 3.1353\n",
            "Epoch [ 6/15], Step [2350/2367], Loss: 2.6070\n",
            "Epoch [ 7/15], Step [ 50/2367], Loss: 3.0361\n",
            "Epoch [ 7/15], Step [100/2367], Loss: 2.8644\n",
            "Epoch [ 7/15], Step [150/2367], Loss: 2.1764\n",
            "Epoch [ 7/15], Step [200/2367], Loss: 1.7469\n",
            "Epoch [ 7/15], Step [250/2367], Loss: 2.3155\n",
            "Epoch [ 7/15], Step [300/2367], Loss: 2.4042\n",
            "Epoch [ 7/15], Step [350/2367], Loss: 2.3600\n",
            "Epoch [ 7/15], Step [400/2367], Loss: 2.4971\n",
            "Epoch [ 7/15], Step [450/2367], Loss: 2.2770\n",
            "Epoch [ 7/15], Step [500/2367], Loss: 2.6954\n",
            "Epoch [ 7/15], Step [550/2367], Loss: 2.6511\n",
            "Epoch [ 7/15], Step [600/2367], Loss: 2.3618\n",
            "Epoch [ 7/15], Step [650/2367], Loss: 2.5572\n",
            "Epoch [ 7/15], Step [700/2367], Loss: 2.7146\n",
            "Epoch [ 7/15], Step [750/2367], Loss: 3.1239\n",
            "Epoch [ 7/15], Step [800/2367], Loss: 2.9005\n",
            "Epoch [ 7/15], Step [850/2367], Loss: 2.4145\n",
            "Epoch [ 7/15], Step [900/2367], Loss: 2.2616\n",
            "Epoch [ 7/15], Step [950/2367], Loss: 2.6101\n",
            "Epoch [ 7/15], Step [1000/2367], Loss: 3.0392\n",
            "Epoch [ 7/15], Step [1050/2367], Loss: 2.1990\n",
            "Epoch [ 7/15], Step [1100/2367], Loss: 2.7215\n",
            "Epoch [ 7/15], Step [1150/2367], Loss: 2.8412\n",
            "Epoch [ 7/15], Step [1200/2367], Loss: 2.7122\n",
            "Epoch [ 7/15], Step [1250/2367], Loss: 2.5599\n",
            "Epoch [ 7/15], Step [1300/2367], Loss: 2.4925\n",
            "Epoch [ 7/15], Step [1350/2367], Loss: 2.5525\n",
            "Epoch [ 7/15], Step [1400/2367], Loss: 2.4468\n",
            "Epoch [ 7/15], Step [1450/2367], Loss: 2.6712\n",
            "Epoch [ 7/15], Step [1500/2367], Loss: 2.7503\n",
            "Epoch [ 7/15], Step [1550/2367], Loss: 3.1644\n",
            "Epoch [ 7/15], Step [1600/2367], Loss: 2.3198\n",
            "Epoch [ 7/15], Step [1650/2367], Loss: 2.5205\n",
            "Epoch [ 7/15], Step [1700/2367], Loss: 2.6676\n",
            "Epoch [ 7/15], Step [1750/2367], Loss: 3.0620\n",
            "Epoch [ 7/15], Step [1800/2367], Loss: 2.6444\n",
            "Epoch [ 7/15], Step [1850/2367], Loss: 2.3522\n",
            "Epoch [ 7/15], Step [1900/2367], Loss: 3.1397\n",
            "Epoch [ 7/15], Step [1950/2367], Loss: 3.2831\n",
            "Epoch [ 7/15], Step [2000/2367], Loss: 2.9045\n",
            "Epoch [ 7/15], Step [2050/2367], Loss: 2.6877\n",
            "Epoch [ 7/15], Step [2100/2367], Loss: 2.6147\n",
            "Epoch [ 7/15], Step [2150/2367], Loss: 2.9895\n",
            "Epoch [ 7/15], Step [2200/2367], Loss: 3.0340\n",
            "Epoch [ 7/15], Step [2250/2367], Loss: 2.6549\n",
            "Epoch [ 7/15], Step [2300/2367], Loss: 2.6136\n",
            "Epoch [ 7/15], Step [2350/2367], Loss: 2.3275\n",
            "Epoch [ 8/15], Step [ 50/2367], Loss: 2.3097\n",
            "Epoch [ 8/15], Step [100/2367], Loss: 2.7687\n",
            "Epoch [ 8/15], Step [150/2367], Loss: 2.7239\n",
            "Epoch [ 8/15], Step [200/2367], Loss: 2.1352\n",
            "Epoch [ 8/15], Step [250/2367], Loss: 2.6977\n",
            "Epoch [ 8/15], Step [300/2367], Loss: 2.4001\n",
            "Epoch [ 8/15], Step [350/2367], Loss: 1.9349\n",
            "Epoch [ 8/15], Step [400/2367], Loss: 2.4704\n",
            "Epoch [ 8/15], Step [450/2367], Loss: 2.1665\n",
            "Epoch [ 8/15], Step [500/2367], Loss: 2.4496\n",
            "Epoch [ 8/15], Step [550/2367], Loss: 2.4168\n",
            "Epoch [ 8/15], Step [600/2367], Loss: 1.8640\n",
            "Epoch [ 8/15], Step [650/2367], Loss: 2.1658\n",
            "Epoch [ 8/15], Step [700/2367], Loss: 2.2797\n",
            "Epoch [ 8/15], Step [750/2367], Loss: 3.2636\n",
            "Epoch [ 8/15], Step [800/2367], Loss: 2.5951\n",
            "Epoch [ 8/15], Step [850/2367], Loss: 2.1748\n",
            "Epoch [ 8/15], Step [900/2367], Loss: 2.5538\n",
            "Epoch [ 8/15], Step [950/2367], Loss: 2.1467\n",
            "Epoch [ 8/15], Step [1000/2367], Loss: 2.8305\n",
            "Epoch [ 8/15], Step [1050/2367], Loss: 2.5703\n",
            "Epoch [ 8/15], Step [1100/2367], Loss: 2.1041\n",
            "Epoch [ 8/15], Step [1150/2367], Loss: 2.6171\n",
            "Epoch [ 8/15], Step [1200/2367], Loss: 2.5321\n",
            "Epoch [ 8/15], Step [1250/2367], Loss: 2.5875\n",
            "Epoch [ 8/15], Step [1300/2367], Loss: 2.5742\n",
            "Epoch [ 8/15], Step [1350/2367], Loss: 2.5470\n",
            "Epoch [ 8/15], Step [1400/2367], Loss: 2.8937\n",
            "Epoch [ 8/15], Step [1450/2367], Loss: 2.5622\n",
            "Epoch [ 8/15], Step [1500/2367], Loss: 2.6532\n",
            "Epoch [ 8/15], Step [1550/2367], Loss: 2.7891\n",
            "Epoch [ 8/15], Step [1600/2367], Loss: 2.3412\n",
            "Epoch [ 8/15], Step [1650/2367], Loss: 2.2523\n",
            "Epoch [ 8/15], Step [1700/2367], Loss: 2.3958\n",
            "Epoch [ 8/15], Step [1750/2367], Loss: 2.9070\n",
            "Epoch [ 8/15], Step [1800/2367], Loss: 2.4877\n",
            "Epoch [ 8/15], Step [1850/2367], Loss: 2.2242\n",
            "Epoch [ 8/15], Step [1900/2367], Loss: 2.4537\n",
            "Epoch [ 8/15], Step [1950/2367], Loss: 1.9811\n",
            "Epoch [ 8/15], Step [2000/2367], Loss: 2.1792\n",
            "Epoch [ 8/15], Step [2050/2367], Loss: 2.2352\n",
            "Epoch [ 8/15], Step [2100/2367], Loss: 2.1878\n",
            "Epoch [ 8/15], Step [2150/2367], Loss: 2.2476\n",
            "Epoch [ 8/15], Step [2200/2367], Loss: 3.1314\n",
            "Epoch [ 8/15], Step [2250/2367], Loss: 2.6254\n",
            "Epoch [ 8/15], Step [2300/2367], Loss: 2.3869\n",
            "Epoch [ 8/15], Step [2350/2367], Loss: 2.8100\n",
            "Epoch [ 9/15], Step [ 50/2367], Loss: 1.8932\n",
            "Epoch [ 9/15], Step [100/2367], Loss: 1.8112\n",
            "Epoch [ 9/15], Step [150/2367], Loss: 2.3002\n",
            "Epoch [ 9/15], Step [200/2367], Loss: 2.5625\n",
            "Epoch [ 9/15], Step [250/2367], Loss: 2.2879\n",
            "Epoch [ 9/15], Step [300/2367], Loss: 2.1247\n",
            "Epoch [ 9/15], Step [350/2367], Loss: 1.9605\n",
            "Epoch [ 9/15], Step [400/2367], Loss: 2.2073\n",
            "Epoch [ 9/15], Step [450/2367], Loss: 1.9062\n",
            "Epoch [ 9/15], Step [500/2367], Loss: 2.5114\n",
            "Epoch [ 9/15], Step [550/2367], Loss: 2.7985\n",
            "Epoch [ 9/15], Step [600/2367], Loss: 2.4165\n",
            "Epoch [ 9/15], Step [650/2367], Loss: 2.5786\n",
            "Epoch [ 9/15], Step [700/2367], Loss: 2.1348\n",
            "Epoch [ 9/15], Step [750/2367], Loss: 2.3501\n",
            "Epoch [ 9/15], Step [800/2367], Loss: 2.7092\n",
            "Epoch [ 9/15], Step [850/2367], Loss: 2.1671\n",
            "Epoch [ 9/15], Step [900/2367], Loss: 2.5162\n",
            "Epoch [ 9/15], Step [950/2367], Loss: 2.7883\n",
            "Epoch [ 9/15], Step [1000/2367], Loss: 2.0937\n",
            "Epoch [ 9/15], Step [1050/2367], Loss: 2.3041\n",
            "Epoch [ 9/15], Step [1100/2367], Loss: 1.9663\n",
            "Epoch [ 9/15], Step [1150/2367], Loss: 2.3363\n",
            "Epoch [ 9/15], Step [1200/2367], Loss: 2.4171\n",
            "Epoch [ 9/15], Step [1250/2367], Loss: 2.3262\n",
            "Epoch [ 9/15], Step [1300/2367], Loss: 2.7447\n",
            "Epoch [ 9/15], Step [1350/2367], Loss: 2.9167\n",
            "Epoch [ 9/15], Step [1400/2367], Loss: 2.8705\n",
            "Epoch [ 9/15], Step [1450/2367], Loss: 2.3388\n",
            "Epoch [ 9/15], Step [1500/2367], Loss: 2.8640\n",
            "Epoch [ 9/15], Step [1550/2367], Loss: 2.4946\n",
            "Epoch [ 9/15], Step [1600/2367], Loss: 1.7967\n",
            "Epoch [ 9/15], Step [1650/2367], Loss: 2.8798\n",
            "Epoch [ 9/15], Step [1700/2367], Loss: 3.0968\n",
            "Epoch [ 9/15], Step [1750/2367], Loss: 2.5717\n",
            "Epoch [ 9/15], Step [1800/2367], Loss: 2.3187\n",
            "Epoch [ 9/15], Step [1850/2367], Loss: 2.0822\n",
            "Epoch [ 9/15], Step [1900/2367], Loss: 2.1156\n",
            "Epoch [ 9/15], Step [1950/2367], Loss: 3.2441\n",
            "Epoch [ 9/15], Step [2000/2367], Loss: 2.5232\n",
            "Epoch [ 9/15], Step [2050/2367], Loss: 2.7688\n",
            "Epoch [ 9/15], Step [2100/2367], Loss: 2.4712\n",
            "Epoch [ 9/15], Step [2150/2367], Loss: 1.9447\n",
            "Epoch [ 9/15], Step [2200/2367], Loss: 2.2440\n",
            "Epoch [ 9/15], Step [2250/2367], Loss: 2.0249\n",
            "Epoch [ 9/15], Step [2300/2367], Loss: 2.3624\n",
            "Epoch [ 9/15], Step [2350/2367], Loss: 2.3461\n",
            "Epoch [10/15], Step [ 50/2367], Loss: 1.6891\n",
            "Epoch [10/15], Step [100/2367], Loss: 2.7837\n",
            "Epoch [10/15], Step [150/2367], Loss: 2.0131\n",
            "Epoch [10/15], Step [200/2367], Loss: 1.7790\n",
            "Epoch [10/15], Step [250/2367], Loss: 1.9881\n",
            "Epoch [10/15], Step [300/2367], Loss: 2.5498\n",
            "Epoch [10/15], Step [350/2367], Loss: 2.4002\n",
            "Epoch [10/15], Step [400/2367], Loss: 2.4788\n",
            "Epoch [10/15], Step [450/2367], Loss: 2.7550\n",
            "Epoch [10/15], Step [500/2367], Loss: 1.8735\n",
            "Epoch [10/15], Step [550/2367], Loss: 2.3704\n",
            "Epoch [10/15], Step [600/2367], Loss: 2.4718\n",
            "Epoch [10/15], Step [650/2367], Loss: 2.3896\n",
            "Epoch [10/15], Step [700/2367], Loss: 2.2122\n",
            "Epoch [10/15], Step [750/2367], Loss: 2.3267\n",
            "Epoch [10/15], Step [800/2367], Loss: 2.3856\n",
            "Epoch [10/15], Step [850/2367], Loss: 2.9645\n",
            "Epoch [10/15], Step [900/2367], Loss: 2.6748\n",
            "Epoch [10/15], Step [950/2367], Loss: 2.3912\n",
            "Epoch [10/15], Step [1000/2367], Loss: 2.9634\n",
            "Epoch [10/15], Step [1050/2367], Loss: 2.3815\n",
            "Epoch [10/15], Step [1100/2367], Loss: 1.6954\n",
            "Epoch [10/15], Step [1150/2367], Loss: 2.5472\n",
            "Epoch [10/15], Step [1200/2367], Loss: 2.3939\n",
            "Epoch [10/15], Step [1250/2367], Loss: 2.2561\n",
            "Epoch [10/15], Step [1300/2367], Loss: 2.4394\n",
            "Epoch [10/15], Step [1350/2367], Loss: 2.1965\n",
            "Epoch [10/15], Step [1400/2367], Loss: 2.4465\n",
            "Epoch [10/15], Step [1450/2367], Loss: 2.6192\n",
            "Epoch [10/15], Step [1500/2367], Loss: 2.5504\n",
            "Epoch [10/15], Step [1550/2367], Loss: 2.0360\n",
            "Epoch [10/15], Step [1600/2367], Loss: 2.2792\n",
            "Epoch [10/15], Step [1650/2367], Loss: 2.5294\n",
            "Epoch [10/15], Step [1700/2367], Loss: 2.3830\n",
            "Epoch [10/15], Step [1750/2367], Loss: 2.3455\n",
            "Epoch [10/15], Step [1800/2367], Loss: 1.6493\n",
            "Epoch [10/15], Step [1850/2367], Loss: 2.1778\n",
            "Epoch [10/15], Step [1900/2367], Loss: 1.7072\n",
            "Epoch [10/15], Step [1950/2367], Loss: 2.1567\n",
            "Epoch [10/15], Step [2000/2367], Loss: 2.7819\n",
            "Epoch [10/15], Step [2050/2367], Loss: 2.1331\n",
            "Epoch [10/15], Step [2100/2367], Loss: 2.7712\n",
            "Epoch [10/15], Step [2150/2367], Loss: 2.7061\n",
            "Epoch [10/15], Step [2200/2367], Loss: 2.7095\n",
            "Epoch [10/15], Step [2250/2367], Loss: 2.6250\n",
            "Epoch [10/15], Step [2300/2367], Loss: 1.6526\n",
            "Epoch [10/15], Step [2350/2367], Loss: 2.3940\n",
            "Epoch [11/15], Step [ 50/2367], Loss: 2.6603\n",
            "Epoch [11/15], Step [100/2367], Loss: 2.2363\n",
            "Epoch [11/15], Step [150/2367], Loss: 2.1425\n",
            "Epoch [11/15], Step [200/2367], Loss: 2.1123\n",
            "Epoch [11/15], Step [250/2367], Loss: 2.4592\n",
            "Epoch [11/15], Step [300/2367], Loss: 1.7389\n",
            "Epoch [11/15], Step [350/2367], Loss: 1.9223\n",
            "Epoch [11/15], Step [400/2367], Loss: 1.9790\n",
            "Epoch [11/15], Step [450/2367], Loss: 2.0719\n",
            "Epoch [11/15], Step [500/2367], Loss: 1.8036\n",
            "Epoch [11/15], Step [550/2367], Loss: 2.0711\n",
            "Epoch [11/15], Step [600/2367], Loss: 1.3436\n",
            "Epoch [11/15], Step [650/2367], Loss: 2.0245\n",
            "Epoch [11/15], Step [700/2367], Loss: 2.2955\n",
            "Epoch [11/15], Step [750/2367], Loss: 2.1215\n",
            "Epoch [11/15], Step [800/2367], Loss: 1.5999\n",
            "Epoch [11/15], Step [850/2367], Loss: 1.6003\n",
            "Epoch [11/15], Step [900/2367], Loss: 1.6034\n",
            "Epoch [11/15], Step [950/2367], Loss: 2.7633\n",
            "Epoch [11/15], Step [1000/2367], Loss: 1.9637\n",
            "Epoch [11/15], Step [1050/2367], Loss: 2.1272\n",
            "Epoch [11/15], Step [1100/2367], Loss: 2.4592\n",
            "Epoch [11/15], Step [1150/2367], Loss: 1.8953\n",
            "Epoch [11/15], Step [1200/2367], Loss: 1.5157\n",
            "Epoch [11/15], Step [1250/2367], Loss: 2.0175\n",
            "Epoch [11/15], Step [1300/2367], Loss: 1.7551\n",
            "Epoch [11/15], Step [1350/2367], Loss: 2.0744\n",
            "Epoch [11/15], Step [1400/2367], Loss: 2.7301\n",
            "Epoch [11/15], Step [1450/2367], Loss: 2.1262\n",
            "Epoch [11/15], Step [1500/2367], Loss: 1.8658\n",
            "Epoch [11/15], Step [1550/2367], Loss: 1.7610\n",
            "Epoch [11/15], Step [1600/2367], Loss: 2.0595\n",
            "Epoch [11/15], Step [1650/2367], Loss: 1.8275\n",
            "Epoch [11/15], Step [1700/2367], Loss: 1.8824\n",
            "Epoch [11/15], Step [1750/2367], Loss: 2.2113\n",
            "Epoch [11/15], Step [1800/2367], Loss: 1.4635\n",
            "Epoch [11/15], Step [1850/2367], Loss: 2.4260\n",
            "Epoch [11/15], Step [1900/2367], Loss: 2.2971\n",
            "Epoch [11/15], Step [1950/2367], Loss: 2.0848\n",
            "Epoch [11/15], Step [2000/2367], Loss: 1.5432\n",
            "Epoch [11/15], Step [2050/2367], Loss: 2.0985\n",
            "Epoch [11/15], Step [2100/2367], Loss: 1.6858\n",
            "Epoch [11/15], Step [2150/2367], Loss: 1.8347\n",
            "Epoch [11/15], Step [2200/2367], Loss: 1.9064\n",
            "Epoch [11/15], Step [2250/2367], Loss: 2.0388\n",
            "Epoch [11/15], Step [2300/2367], Loss: 2.2143\n",
            "Epoch [11/15], Step [2350/2367], Loss: 1.9205\n",
            "Epoch [12/15], Step [ 50/2367], Loss: 1.3529\n",
            "Epoch [12/15], Step [100/2367], Loss: 0.9852\n",
            "Epoch [12/15], Step [150/2367], Loss: 1.4265\n",
            "Epoch [12/15], Step [200/2367], Loss: 1.2151\n",
            "Epoch [12/15], Step [250/2367], Loss: 2.1462\n",
            "Epoch [12/15], Step [300/2367], Loss: 1.8314\n",
            "Epoch [12/15], Step [350/2367], Loss: 1.8628\n",
            "Epoch [12/15], Step [400/2367], Loss: 2.1152\n",
            "Epoch [12/15], Step [450/2367], Loss: 1.7320\n",
            "Epoch [12/15], Step [500/2367], Loss: 1.3796\n",
            "Epoch [12/15], Step [550/2367], Loss: 1.3903\n",
            "Epoch [12/15], Step [600/2367], Loss: 2.5500\n",
            "Epoch [12/15], Step [650/2367], Loss: 2.5584\n",
            "Epoch [12/15], Step [700/2367], Loss: 1.5166\n",
            "Epoch [12/15], Step [750/2367], Loss: 2.0365\n",
            "Epoch [12/15], Step [800/2367], Loss: 1.8596\n",
            "Epoch [12/15], Step [850/2367], Loss: 2.1493\n",
            "Epoch [12/15], Step [900/2367], Loss: 1.7203\n",
            "Epoch [12/15], Step [950/2367], Loss: 1.6579\n",
            "Epoch [12/15], Step [1000/2367], Loss: 1.8090\n",
            "Epoch [12/15], Step [1050/2367], Loss: 1.9080\n",
            "Epoch [12/15], Step [1100/2367], Loss: 2.2869\n",
            "Epoch [12/15], Step [1150/2367], Loss: 2.6006\n",
            "Epoch [12/15], Step [1200/2367], Loss: 1.5301\n",
            "Epoch [12/15], Step [1250/2367], Loss: 1.9206\n",
            "Epoch [12/15], Step [1300/2367], Loss: 2.0888\n",
            "Epoch [12/15], Step [1350/2367], Loss: 2.2397\n",
            "Epoch [12/15], Step [1400/2367], Loss: 1.6784\n",
            "Epoch [12/15], Step [1450/2367], Loss: 2.2347\n",
            "Epoch [12/15], Step [1500/2367], Loss: 2.1104\n",
            "Epoch [12/15], Step [1550/2367], Loss: 2.2264\n",
            "Epoch [12/15], Step [1600/2367], Loss: 1.9528\n",
            "Epoch [12/15], Step [1650/2367], Loss: 2.1626\n",
            "Epoch [12/15], Step [1700/2367], Loss: 1.5506\n",
            "Epoch [12/15], Step [1750/2367], Loss: 1.7777\n",
            "Epoch [12/15], Step [1800/2367], Loss: 2.2781\n",
            "Epoch [12/15], Step [1850/2367], Loss: 1.6320\n",
            "Epoch [12/15], Step [1900/2367], Loss: 2.2029\n",
            "Epoch [12/15], Step [1950/2367], Loss: 1.4927\n",
            "Epoch [12/15], Step [2000/2367], Loss: 1.8211\n",
            "Epoch [12/15], Step [2050/2367], Loss: 1.7572\n",
            "Epoch [12/15], Step [2100/2367], Loss: 2.0175\n",
            "Epoch [12/15], Step [2150/2367], Loss: 1.9204\n",
            "Epoch [12/15], Step [2200/2367], Loss: 2.6009\n",
            "Epoch [12/15], Step [2250/2367], Loss: 1.9221\n",
            "Epoch [12/15], Step [2300/2367], Loss: 1.6162\n",
            "Epoch [12/15], Step [2350/2367], Loss: 2.0066\n",
            "Epoch [13/15], Step [ 50/2367], Loss: 1.1110\n",
            "Epoch [13/15], Step [100/2367], Loss: 1.3191\n",
            "Epoch [13/15], Step [150/2367], Loss: 2.2021\n",
            "Epoch [13/15], Step [200/2367], Loss: 1.8280\n",
            "Epoch [13/15], Step [250/2367], Loss: 1.5791\n",
            "Epoch [13/15], Step [300/2367], Loss: 1.4362\n",
            "Epoch [13/15], Step [350/2367], Loss: 1.6931\n",
            "Epoch [13/15], Step [400/2367], Loss: 1.7203\n",
            "Epoch [13/15], Step [450/2367], Loss: 1.4439\n",
            "Epoch [13/15], Step [500/2367], Loss: 1.8048\n",
            "Epoch [13/15], Step [550/2367], Loss: 1.6790\n",
            "Epoch [13/15], Step [600/2367], Loss: 1.7476\n",
            "Epoch [13/15], Step [650/2367], Loss: 2.5473\n",
            "Epoch [13/15], Step [700/2367], Loss: 2.0045\n",
            "Epoch [13/15], Step [750/2367], Loss: 2.4973\n",
            "Epoch [13/15], Step [800/2367], Loss: 2.3691\n",
            "Epoch [13/15], Step [850/2367], Loss: 2.8691\n",
            "Epoch [13/15], Step [900/2367], Loss: 2.0245\n",
            "Epoch [13/15], Step [950/2367], Loss: 1.2430\n",
            "Epoch [13/15], Step [1000/2367], Loss: 2.0101\n",
            "Epoch [13/15], Step [1050/2367], Loss: 2.4724\n",
            "Epoch [13/15], Step [1100/2367], Loss: 2.1997\n",
            "Epoch [13/15], Step [1150/2367], Loss: 2.3275\n",
            "Epoch [13/15], Step [1200/2367], Loss: 1.9521\n",
            "Epoch [13/15], Step [1250/2367], Loss: 2.0259\n",
            "Epoch [13/15], Step [1300/2367], Loss: 1.6568\n",
            "Epoch [13/15], Step [1350/2367], Loss: 1.8357\n",
            "Epoch [13/15], Step [1400/2367], Loss: 1.5003\n",
            "Epoch [13/15], Step [1450/2367], Loss: 1.8863\n",
            "Epoch [13/15], Step [1500/2367], Loss: 2.2868\n",
            "Epoch [13/15], Step [1550/2367], Loss: 1.9445\n",
            "Epoch [13/15], Step [1600/2367], Loss: 2.4127\n",
            "Epoch [13/15], Step [1650/2367], Loss: 2.2265\n",
            "Epoch [13/15], Step [1700/2367], Loss: 1.9702\n",
            "Epoch [13/15], Step [1750/2367], Loss: 1.6558\n",
            "Epoch [13/15], Step [1800/2367], Loss: 2.1068\n",
            "Epoch [13/15], Step [1850/2367], Loss: 2.3530\n",
            "Epoch [13/15], Step [1900/2367], Loss: 2.4159\n",
            "Epoch [13/15], Step [1950/2367], Loss: 1.8098\n",
            "Epoch [13/15], Step [2000/2367], Loss: 1.6736\n",
            "Epoch [13/15], Step [2050/2367], Loss: 1.9912\n",
            "Epoch [13/15], Step [2100/2367], Loss: 2.9790\n",
            "Epoch [13/15], Step [2150/2367], Loss: 2.0090\n",
            "Epoch [13/15], Step [2200/2367], Loss: 2.1671\n",
            "Epoch [13/15], Step [2250/2367], Loss: 2.1450\n",
            "Epoch [13/15], Step [2300/2367], Loss: 2.5588\n",
            "Epoch [13/15], Step [2350/2367], Loss: 2.3420\n",
            "Epoch [14/15], Step [ 50/2367], Loss: 1.5899\n",
            "Epoch [14/15], Step [100/2367], Loss: 1.4202\n",
            "Epoch [14/15], Step [150/2367], Loss: 1.5415\n",
            "Epoch [14/15], Step [200/2367], Loss: 1.5758\n",
            "Epoch [14/15], Step [250/2367], Loss: 1.9948\n",
            "Epoch [14/15], Step [300/2367], Loss: 1.7751\n",
            "Epoch [14/15], Step [350/2367], Loss: 1.8748\n",
            "Epoch [14/15], Step [400/2367], Loss: 1.8773\n",
            "Epoch [14/15], Step [450/2367], Loss: 2.0122\n",
            "Epoch [14/15], Step [500/2367], Loss: 1.5861\n",
            "Epoch [14/15], Step [550/2367], Loss: 2.7231\n",
            "Epoch [14/15], Step [600/2367], Loss: 1.4583\n",
            "Epoch [14/15], Step [650/2367], Loss: 1.9588\n",
            "Epoch [14/15], Step [700/2367], Loss: 2.3097\n",
            "Epoch [14/15], Step [750/2367], Loss: 2.0646\n",
            "Epoch [14/15], Step [800/2367], Loss: 2.1400\n",
            "Epoch [14/15], Step [850/2367], Loss: 1.8614\n",
            "Epoch [14/15], Step [900/2367], Loss: 1.9963\n",
            "Epoch [14/15], Step [950/2367], Loss: 1.5888\n",
            "Epoch [14/15], Step [1000/2367], Loss: 1.5881\n",
            "Epoch [14/15], Step [1050/2367], Loss: 2.1681\n",
            "Epoch [14/15], Step [1100/2367], Loss: 2.0978\n",
            "Epoch [14/15], Step [1150/2367], Loss: 2.2485\n",
            "Epoch [14/15], Step [1200/2367], Loss: 1.3297\n",
            "Epoch [14/15], Step [1250/2367], Loss: 2.0299\n",
            "Epoch [14/15], Step [1300/2367], Loss: 1.6868\n",
            "Epoch [14/15], Step [1350/2367], Loss: 1.8021\n",
            "Epoch [14/15], Step [1400/2367], Loss: 1.8274\n",
            "Epoch [14/15], Step [1450/2367], Loss: 1.9604\n",
            "Epoch [14/15], Step [1500/2367], Loss: 1.5280\n",
            "Epoch [14/15], Step [1550/2367], Loss: 1.4715\n",
            "Epoch [14/15], Step [1600/2367], Loss: 1.6134\n",
            "Epoch [14/15], Step [1650/2367], Loss: 2.2267\n",
            "Epoch [14/15], Step [1700/2367], Loss: 1.7040\n",
            "Epoch [14/15], Step [1750/2367], Loss: 1.6940\n",
            "Epoch [14/15], Step [1800/2367], Loss: 2.0173\n",
            "Epoch [14/15], Step [1850/2367], Loss: 1.7087\n",
            "Epoch [14/15], Step [1900/2367], Loss: 1.7959\n",
            "Epoch [14/15], Step [1950/2367], Loss: 2.3043\n",
            "Epoch [14/15], Step [2000/2367], Loss: 2.2013\n",
            "Epoch [14/15], Step [2050/2367], Loss: 2.2510\n",
            "Epoch [14/15], Step [2100/2367], Loss: 2.5031\n",
            "Epoch [14/15], Step [2150/2367], Loss: 2.1431\n",
            "Epoch [14/15], Step [2200/2367], Loss: 2.1820\n",
            "Epoch [14/15], Step [2250/2367], Loss: 2.1406\n",
            "Epoch [14/15], Step [2300/2367], Loss: 1.7556\n",
            "Epoch [14/15], Step [2350/2367], Loss: 1.5073\n",
            "Epoch [15/15], Step [ 50/2367], Loss: 1.1397\n",
            "Epoch [15/15], Step [100/2367], Loss: 1.4895\n",
            "Epoch [15/15], Step [150/2367], Loss: 1.2944\n",
            "Epoch [15/15], Step [200/2367], Loss: 1.2145\n",
            "Epoch [15/15], Step [250/2367], Loss: 1.3867\n",
            "Epoch [15/15], Step [300/2367], Loss: 1.1525\n",
            "Epoch [15/15], Step [350/2367], Loss: 1.2045\n",
            "Epoch [15/15], Step [400/2367], Loss: 1.2457\n",
            "Epoch [15/15], Step [450/2367], Loss: 1.4858\n",
            "Epoch [15/15], Step [500/2367], Loss: 1.9932\n",
            "Epoch [15/15], Step [550/2367], Loss: 1.8502\n",
            "Epoch [15/15], Step [600/2367], Loss: 1.7685\n",
            "Epoch [15/15], Step [650/2367], Loss: 1.3346\n",
            "Epoch [15/15], Step [700/2367], Loss: 1.0989\n",
            "Epoch [15/15], Step [750/2367], Loss: 1.4214\n",
            "Epoch [15/15], Step [800/2367], Loss: 1.3744\n",
            "Epoch [15/15], Step [850/2367], Loss: 1.4823\n",
            "Epoch [15/15], Step [900/2367], Loss: 1.8060\n",
            "Epoch [15/15], Step [950/2367], Loss: 1.4200\n",
            "Epoch [15/15], Step [1000/2367], Loss: 1.8258\n",
            "Epoch [15/15], Step [1050/2367], Loss: 1.7916\n",
            "Epoch [15/15], Step [1100/2367], Loss: 1.5659\n",
            "Epoch [15/15], Step [1150/2367], Loss: 2.0393\n",
            "Epoch [15/15], Step [1200/2367], Loss: 1.7281\n",
            "Epoch [15/15], Step [1250/2367], Loss: 1.7836\n",
            "Epoch [15/15], Step [1300/2367], Loss: 1.4058\n",
            "Epoch [15/15], Step [1350/2367], Loss: 1.6998\n",
            "Epoch [15/15], Step [1400/2367], Loss: 1.9299\n",
            "Epoch [15/15], Step [1450/2367], Loss: 1.5061\n",
            "Epoch [15/15], Step [1500/2367], Loss: 1.7179\n",
            "Epoch [15/15], Step [1550/2367], Loss: 1.5323\n",
            "Epoch [15/15], Step [1600/2367], Loss: 1.7347\n",
            "Epoch [15/15], Step [1650/2367], Loss: 1.3675\n",
            "Epoch [15/15], Step [1700/2367], Loss: 1.8679\n",
            "Epoch [15/15], Step [1750/2367], Loss: 1.2721\n",
            "Epoch [15/15], Step [1800/2367], Loss: 1.8108\n",
            "Epoch [15/15], Step [1850/2367], Loss: 1.6017\n",
            "Epoch [15/15], Step [1900/2367], Loss: 1.4534\n",
            "Epoch [15/15], Step [1950/2367], Loss: 1.6798\n",
            "Epoch [15/15], Step [2000/2367], Loss: 1.4582\n",
            "Epoch [15/15], Step [2050/2367], Loss: 1.1359\n",
            "Epoch [15/15], Step [2100/2367], Loss: 1.8739\n",
            "Epoch [15/15], Step [2150/2367], Loss: 1.8190\n",
            "Epoch [15/15], Step [2200/2367], Loss: 1.9062\n",
            "Epoch [15/15], Step [2250/2367], Loss: 1.7625\n",
            "Epoch [15/15], Step [2300/2367], Loss: 1.0361\n",
            "Epoch [15/15], Step [2350/2367], Loss: 2.5869\n",
            "CPU times: user 1h 14min 21s, sys: 45.6 s, total: 1h 15min 7s\n",
            "Wall time: 1h 30min 57s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(losses)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('loss')\n",
        "plt.title('Cross Entropy Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "LIZhbJzcCTWM",
        "outputId": "9b9acbda-3d2c-4272-ad64-bf1930bcab1b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, dataloader, device):\n",
        "    model.eval()  # for batch normalization layers\n",
        "    corrects = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            corrects += (preds == targets.data).sum()\n",
        "\n",
        "    print('accuracy: {:.2f}'.format(100. * corrects / len(dataloader.dataset)))"
      ],
      "metadata": {
        "id": "ykVFEMuXCTbb"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(model, train_dataloader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taqrBORQCTmx",
        "outputId": "ea9b36ca-8b97-482e-cb3c-16ab9d1c39a6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 74.50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(model, evaluation_dataloader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrCrkVEiCYrt",
        "outputId": "f5df134c-5b4b-4ee8-daa7-1050111d5ccc"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 27.45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models import resnet101\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),       # ResNet-101 uses 224x224 images\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Load the dataset\n",
        "# train_set = torchvision.datasets.ImageFolder(root='./food-101/images/train', transform=transform)\n",
        "# test_set = torchvision.datasets.ImageFolder(root='./food-101/images/test', transform=transform)\n",
        "\n",
        "train_set = datasets.Food101(\n",
        "    root=\"data\",\n",
        "    split='train',\n",
        "    download=True,\n",
        "    transform=transformations\n",
        ")\n",
        "\n",
        "test_set = datasets.Food101(\n",
        "    root=\"data\",\n",
        "    split='test',\n",
        "    download=True,\n",
        "    transform=transformations\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_set, batch_size=32, shuffle=False)\n",
        "\n",
        "# Load the pre-trained ResNet-101 model\n",
        "model = resnet101(pretrained=True)\n",
        "\n",
        "# Modify the last fully connected layer\n",
        "model.fc = nn.Linear(model.fc.in_features, 101) # Assuming 101 classes\n",
        "\n",
        "# Move the model to the device (GPU or CPU)\n",
        "model = model.to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:    # print every 100 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# Evaluate the model\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the test images: {100 * correct // total} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnxjWK2CZiNx",
        "outputId": "13248a3a-c670-4bc2-c352-8620a2e40836"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://data.vision.ee.ethz.ch/cvl/food-101.tar.gz to data/food-101.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4996278331/4996278331 [04:50<00:00, 17203719.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/food-101.tar.gz to data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 171M/171M [00:01<00:00, 146MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   100] loss: 4.976\n",
            "[1,   200] loss: 4.926\n",
            "[1,   300] loss: 4.867\n",
            "[1,   400] loss: 4.851\n",
            "[1,   500] loss: 4.769\n",
            "[1,   600] loss: 4.707\n",
            "[1,   700] loss: 4.629\n",
            "[1,   800] loss: 4.604\n",
            "[1,   900] loss: 4.515\n",
            "[1,  1000] loss: 4.474\n",
            "[1,  1100] loss: 4.479\n",
            "[1,  1200] loss: 4.409\n",
            "[1,  1300] loss: 4.443\n",
            "[1,  1400] loss: 4.598\n",
            "[1,  1500] loss: 4.503\n",
            "[1,  1600] loss: 4.465\n",
            "[1,  1700] loss: 4.435\n",
            "[1,  1800] loss: 4.382\n",
            "[1,  1900] loss: 4.348\n",
            "[1,  2000] loss: 4.317\n",
            "[1,  2100] loss: 4.299\n",
            "[1,  2200] loss: 4.288\n",
            "[1,  2300] loss: 4.283\n",
            "[2,   100] loss: 4.229\n",
            "[2,   200] loss: 4.259\n",
            "[2,   300] loss: 4.187\n",
            "[2,   400] loss: 4.177\n",
            "[2,   500] loss: 4.165\n",
            "[2,   600] loss: 4.147\n",
            "[2,   700] loss: 4.137\n",
            "[2,   800] loss: 4.113\n",
            "[2,   900] loss: 4.137\n",
            "[2,  1000] loss: 4.145\n",
            "[2,  1100] loss: 4.120\n",
            "[2,  1200] loss: 4.076\n",
            "[2,  1300] loss: 4.063\n",
            "[2,  1400] loss: 4.072\n",
            "[2,  1500] loss: 4.062\n",
            "[2,  1600] loss: 4.045\n",
            "[2,  1700] loss: 4.029\n",
            "[2,  1800] loss: 4.018\n",
            "[2,  1900] loss: 4.072\n",
            "[2,  2000] loss: 3.996\n",
            "[2,  2100] loss: 3.985\n",
            "[2,  2200] loss: 3.978\n",
            "[2,  2300] loss: 4.026\n",
            "[3,   100] loss: 3.918\n",
            "[3,   200] loss: 3.919\n",
            "[3,   300] loss: 3.947\n",
            "[3,   400] loss: 3.911\n",
            "[3,   500] loss: 3.919\n",
            "[3,   600] loss: 3.870\n",
            "[3,   700] loss: 4.000\n",
            "[3,   800] loss: 3.925\n",
            "[3,   900] loss: 3.847\n",
            "[3,  1000] loss: 3.852\n",
            "[3,  1100] loss: 3.839\n",
            "[3,  1200] loss: 3.823\n",
            "[3,  1300] loss: 4.067\n",
            "[3,  1400] loss: 3.980\n",
            "[3,  1500] loss: 4.005\n",
            "[3,  1600] loss: 3.986\n",
            "[3,  1700] loss: 3.969\n",
            "[3,  1800] loss: 3.955\n",
            "[3,  1900] loss: 3.865\n",
            "[3,  2000] loss: 3.999\n",
            "[3,  2100] loss: 3.891\n",
            "[3,  2200] loss: 3.856\n",
            "[3,  2300] loss: 3.905\n",
            "[4,   100] loss: 4.070\n",
            "[4,   200] loss: 3.865\n",
            "[4,   300] loss: 3.819\n",
            "[4,   400] loss: 3.814\n",
            "[4,   500] loss: 3.809\n",
            "[4,   600] loss: 3.805\n",
            "[4,   700] loss: 3.789\n",
            "[4,   800] loss: 3.822\n",
            "[4,   900] loss: 3.700\n",
            "[4,  1000] loss: 3.799\n",
            "[4,  1100] loss: 3.922\n",
            "[4,  1200] loss: 4.006\n",
            "[4,  1300] loss: 4.000\n",
            "[4,  1400] loss: 3.863\n",
            "[4,  1500] loss: 3.893\n",
            "[4,  1600] loss: 3.853\n",
            "[4,  1700] loss: 3.902\n",
            "[4,  1800] loss: 3.790\n",
            "[4,  1900] loss: 3.777\n",
            "[4,  2000] loss: 3.797\n",
            "[4,  2100] loss: 3.764\n",
            "[4,  2200] loss: 3.755\n",
            "[4,  2300] loss: 3.841\n",
            "[5,   100] loss: 3.722\n",
            "[5,   200] loss: 3.700\n",
            "[5,   300] loss: 3.662\n",
            "[5,   400] loss: 3.710\n",
            "[5,   500] loss: 3.801\n",
            "[5,   600] loss: 3.808\n",
            "[5,   700] loss: 3.809\n",
            "[5,   800] loss: 3.813\n",
            "[5,   900] loss: 3.700\n",
            "[5,  1000] loss: 3.711\n",
            "[5,  1100] loss: 3.695\n",
            "[5,  1200] loss: 3.705\n",
            "[5,  1300] loss: 3.697\n",
            "[5,  1400] loss: 3.633\n",
            "[5,  1500] loss: 3.643\n",
            "[5,  1600] loss: 3.624\n",
            "[5,  1700] loss: 3.633\n",
            "[5,  1800] loss: 3.684\n",
            "[5,  1900] loss: 3.659\n",
            "[5,  2000] loss: 3.717\n",
            "[5,  2100] loss: 3.672\n",
            "[5,  2200] loss: 3.697\n",
            "[5,  2300] loss: 3.612\n",
            "Finished Training\n",
            "Accuracy of the network on the test images: 16 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "# Define transforms with data augmentation for the training set\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Use a less aggressive transform for the validation set\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Load the modified dataset with the transforms\n",
        "train_set = datasets.Food101(\n",
        "    root=\"data\",\n",
        "    split='train',\n",
        "    download=True,\n",
        "    transform=transformations\n",
        ")\n",
        "\n",
        "test_set = datasets.Food101(\n",
        "    root=\"data\",\n",
        "    split='test',\n",
        "    download=True,\n",
        "    transform=transformations\n",
        ")\n",
        "\n",
        "# ... (rest of the dataset loading code)\n",
        "\n",
        "# Freeze earlier layers of the pre-trained model\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Unfreeze the later layers and the newly added fully connected layer\n",
        "for param in model.layer4.parameters():\n",
        "    param.requires_grad = True\n",
        "model.fc.requires_grad = True\n",
        "\n",
        "# ... (rest of the model setup code)\n",
        "\n",
        "# Add weight decay for regularization\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001, weight_decay=1e-5)\n",
        "\n",
        "# Learning rate scheduler setup\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2)\n",
        "\n",
        "# Implement early stopping\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_acc = 0.0\n",
        "early_stopping_counter = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # ... (training loop code)\n",
        "\n",
        "    # Validation phase\n",
        "    val_loss = 0.0\n",
        "    val_corrects = 0\n",
        "    for inputs, labels in test_loader:\n",
        "        model.eval()\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "            val_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    val_loss = val_loss / len(test_set)\n",
        "    val_acc = val_corrects.double() / len(test_set)\n",
        "\n",
        "    print(f'Validation loss: {val_loss:.4f}, Acc: {val_acc:.4f}')\n",
        "\n",
        "    # Deep copy the model if it has the best accuracy so far\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        early_stopping_counter = 0\n",
        "    else:\n",
        "        early_stopping_counter += 1\n",
        "\n",
        "    # Early stopping\n",
        "    if early_stopping_counter >= 5:\n",
        "        print(\"Early stopping\")\n",
        "        break\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "# Load best model weights\n",
        "model.load_state_dict(best_model_wts)\n",
        "\n",
        "print(f'Best val Acc: {best_acc:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnrI4wMXoC6q",
        "outputId": "2970b8a1-a7f3-4d6b-f405-0f7b3704c827"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 3.6344, Acc: 0.1658\n",
            "Validation loss: 3.6344, Acc: 0.1658\n",
            "Validation loss: 3.6344, Acc: 0.1658\n",
            "Validation loss: 3.6344, Acc: 0.1658\n",
            "Validation loss: 3.6344, Acc: 0.1658\n",
            "Best val Acc: 0.1658\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "def find_learning_rate(model, train_loader, criterion, optimizer, init_value=1e-8, final_value=10., beta=0.98):\n",
        "    num = len(train_loader) - 1\n",
        "    mult = (final_value / init_value) ** (1/num)\n",
        "    lr = init_value\n",
        "    optimizer.param_groups[0]['lr'] = lr\n",
        "    avg_loss = 0.\n",
        "    best_loss = 0.\n",
        "    batch_num = 0\n",
        "    losses = []\n",
        "    log_lrs = []\n",
        "    for data in train_loader:\n",
        "        batch_num += 1\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Compute the smoothed loss\n",
        "        avg_loss = beta * avg_loss + (1-beta) * loss.item()\n",
        "        smoothed_loss = avg_loss / (1 - beta**batch_num)\n",
        "\n",
        "        # Stop if the loss is exploding\n",
        "        if batch_num > 1 and smoothed_loss > 4 * best_loss:\n",
        "            return log_lrs, losses\n",
        "\n",
        "        # Record the best loss\n",
        "        if smoothed_loss < best_loss or batch_num==1:\n",
        "            best_loss = smoothed_loss\n",
        "\n",
        "        # Store the values\n",
        "        losses.append(smoothed_loss)\n",
        "        log_lrs.append(math.log10(lr))\n",
        "\n",
        "        # Do the backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the lr for the next step\n",
        "        lr *= mult\n",
        "        optimizer.param_groups[0]['lr'] = lr\n",
        "\n",
        "    return log_lrs, losses\n",
        "\n",
        "# Run the learning rate finder\n",
        "log_lrs, losses = find_learning_rate(model, train_loader, criterion, optimizer)\n",
        "# Plot the loss vs log-lr graph to find the optimal learning rate"
      ],
      "metadata": {
        "id": "DEQ7KXXTw9Qz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in model.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Train for a few epochs, then unfreeze the next layer\n",
        "num_epochs_per_layer = 1\n",
        "for layer in reversed(list(model.children())):\n",
        "    for param in layer.parameters():\n",
        "        param.requires_grad = True\n",
        "    for epoch in range(num_epochs_per_layer):\n",
        "        # Perform training on the unfrozen layers\n",
        "        pass  # Replace with actual training code\n",
        "\n",
        "    num_epochs_per_layer += 1  # Optionally increase the number of epochs per layer\n"
      ],
      "metadata": {
        "id": "k0ITopzHxC5Q"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import resnet50\n",
        "\n",
        "# Load the pre-trained ResNet-50 model\n",
        "model = resnet50(pretrained=True)\n",
        "# Modify the last fully connected layer\n",
        "model.fc = nn.Linear(model.fc.in_features, 101) # Assuming 101 classes"
      ],
      "metadata": {
        "id": "r5b-GriCxC__",
        "outputId": "33c0c30c-ac0b-4fce-ad33-cf7f6c4f48e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97.8M/97.8M [00:01<00:00, 91.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ... (rest of the model setup code)\n",
        "\n",
        "# Add weight decay for regularization\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001, weight_decay=1e-5)\n",
        "\n",
        "# Learning rate scheduler setup\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2)\n",
        "\n",
        "# Implement early stopping\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_acc = 0.0\n",
        "early_stopping_counter = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # ... (training loop code)\n",
        "\n",
        "    # Validation phase\n",
        "    val_loss = 0.0\n",
        "    val_corrects = 0\n",
        "    for inputs, labels in test_loader:\n",
        "        model.eval()\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "            val_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    val_loss = val_loss / len(test_set)\n",
        "    val_acc = val_corrects.double() / len(test_set)\n",
        "\n",
        "    print(f'Validation loss: {val_loss:.4f}, Acc: {val_acc:.4f}')\n",
        "\n",
        "    # Deep copy the model if it has the best accuracy so far\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        early_stopping_counter = 0\n",
        "    else:\n",
        "        early_stopping_counter += 1\n",
        "\n",
        "    # Early stopping\n",
        "    if early_stopping_counter >= 5:\n",
        "        print(\"Early stopping\")\n",
        "        break\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "# Load best model weights\n",
        "model.load_state_dict(best_model_wts)\n",
        "\n",
        "print(f'Best val Acc: {best_acc:.4f}')"
      ],
      "metadata": {
        "id": "BiNbA8sDxDFp",
        "outputId": "227d372d-ea71-45f6-ec56-885b759fd2e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-92c8843f1756>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;31m# See note [TorchScript super()]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    454\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 456\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    457\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RNvZ4nkOxDIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uPrlXeEfxDLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lT5NxrHkxDPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "projects/simple_nn.ipynb"
      ],
      "metadata": {
        "id": "PJrhDWp3LSHe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}